---
title: "RandomForest_FinalVersion"
author: "Sivhuo Prak (Siv)"
date: "12/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

## Load the Libraries

These are the libraries needed to build the model and execute the code below. 

```{r}
library(tidyverse)         # for reading in data, graphing, and cleaning
library(tidymodels)        # for modeling ... tidily
library(stacks)            # for stacking models
library(glmnet)            # for regularized regression, including LASSO
library(ranger)            # for random forest model
library(kknn)              # for knn model
library(naniar)            # for examining missing values (NAs)
library(lubridate)         # for date manipulation
library(vip)               # for variable importance plots
library(rmarkdown)         # for paged tables
library(usemodels)         # for suggesting step_XXX() functions
library(naniar)            # for examining missing values (NAs)
library(readr)
library(fastDummies)
library(kableExtra)
library(DALEX)             # moDel Agnostic Language for Exploration and eXplanation (for model interpretation)  
library(DALEXtra)
```

```{r}
theme_set(theme_minimal()) 
```

## Data Processing

```{r}
finalDATASET <- read_csv("FINALDATASET.csv")

final_data <- finalDATASET %>% 
  dummy_cols(select_columns = "Sector") %>% 
  mutate(across(c("CPALTT01USM657N_PC1","GDP","GDP_PC1","T10Y2Y","M1SL_PC1","M1SL"),
                as.numeric)) %>% 
  mutate(across(c(DEBTS,INVESTMENTS,CASH,VOLUME,EARNINGS,COGS,SALES,RECEIVABLE,INVENTORY),~replace(.,.==0,NA))) %>% 
  mutate(across(c(!where(is.numeric),-"COMPANY"),as.factor)) 
```

```{r}
final_data <- final_data %>% 
  select(-Earnings_next_year,-PRICE,-Sell)
```

```{r}
final_data$EARNINGS[is.na(final_data$EARNINGS)]<-median(final_data$EARNINGS,na.rm=TRUE)
final_data$INVESTMENTS[is.na(final_data$INVESTMENTS)]<-median(final_data$INVESTMENTS,na.rm=TRUE)
final_data$DEBTS[is.na(final_data$DEBTS)]<-median(final_data$DEBTS,na.rm=TRUE)
final_data$COGS[is.na(final_data$COGS)]<-median(final_data$COGS,na.rm=TRUE)
final_data$SALES[is.na(final_data$SALES)]<-median(final_data$SALES,na.rm=TRUE)
final_data$CASH[is.na(final_data$CASH)]<-median(final_data$CASH,na.rm=TRUE)
final_data$RECEIVABLE[is.na(final_data$RECEIVABLE)]<-median(final_data$RECEIVABLE,na.rm=TRUE)
```

```{r}
final_data <- final_data %>% 
  drop_na()
```

## Splitting Data

The dataset is split into two portions (`data_training` and `data_testing`) for training and testing the models. The function below is randomly assigns 75% of the data to training, and 25% for testing. 

```{r}
set.seed(327) #for reproducibility

# Randomly assigns 75% of the data to training.
data_split <- initial_split(final_data, 
                             prop = .75)
data_training <- training(data_split)
data_testing <- testing(data_split)
```

## Building the Random Forest model

To build the random forest model, we set up recipe using our training data (`data_training`), define model with `mtry = 10`, `min_n = 10` and `numbers of tree = 200`, create ranger workflow, and then we can fit the model. 

```{r}
# set up recipe 
ranger_recipe <-
  recipe(PROFIT ~ ., #short-cut, . = all other vars
                       data = data_training) %>%
  step_filter(YEAR<2021) %>% 
  # remove the unwanted variables
  step_rm(YEAR,COMPANY,Name,GDP,M1SL,Sector) %>% 
  # add PE 
  step_mutate(PE = `MARKET CAP`/EARNINGS)

#define model
ranger_spec <- 
  rand_forest(mtry = 10, 
              min_n = 10, 
              trees = 200) %>% 
  set_mode("regression") %>% 
  set_engine("ranger")

#create workflow
ranger_workflow <- 
  workflow() %>% 
  add_recipe(ranger_recipe) %>% 
  add_model(ranger_spec)

#fit the model
set.seed(712) # for reproducibility - random sampling in random forest choosing number of variables
ranger_fit <- ranger_workflow %>% 
  fit(data_training)
```

## Random Forest results

The table below shows the OOB error (MSE), OOB RMSE (Root mean square error), and R squared of the stock return predictions using the random forest model above.

```{r}
# OOB error (MSE) ... 
ranger_fit$fit$fit$fit$prediction.error
#OOB RMSE
sqrt(ranger_fit$fit$fit$fit$prediction.error)
# R squared
ranger_fit$fit$fit$fit$r.squared
```

## Model Evaluation
### Prediciton precision (Traning data)

Even though the mean rmse from the random forest (38.01155) seems to be fairly high but it is still lower than the LASSO model. Thus, random forest model performs better. 

```{r}
set.seed(1211) # for reproducibility
data_cv <- vfold_cv(data_training, v = 5)

metric <- metric_set(rmse)
ctrl_res <- control_stack_resamples()

ranger_cv <- ranger_workflow %>% 
  fit_resamples(data_cv, 
                metrics = metric,
                control = ctrl_res)

# Evaluation metrics averaged over all folds:
collect_metrics(ranger_cv)
```
We also plot a graph showing the actual return vs predicted return. 

```{r}
ranger_prediction <- predict(
  ranger_fit,
  new_data = data_training)

ranger_training_pred<-data_training %>% 
  mutate(.pred = ranger_prediction$.pred)
  
  
ranger_training_pred %>%
  ggplot(aes(x = PROFIT,
             y = .pred)) +
  geom_point(alpha = .5,
             size = .5) +
  geom_smooth(se = FALSE) +
  geom_abline(slope = 1,
              intercept = 0,
              color = "darkred") +
  labs(x = "Actual Return",
       y = "Predicted Return")

```
### Prediciton precision (Testing data)

The table below shows the rmse on testing data. It is a bit higher but fairly similar to the error on training data so we could say that our model did not overfit. 

```{r}
set.seed(1211) # for reproducibility
data_cv <- vfold_cv(data_testing, v = 5)

metric <- metric_set(rmse)
ctrl_res <- control_stack_resamples()

ranger_cv <- ranger_workflow %>% 
  fit_resamples(data_cv, 
                metrics = metric,
                control = ctrl_res)

# Evaluation metrics averaged over all folds:
collect_metrics(ranger_cv)
```
Below is the graph showing the actual return vs. predicted return on testing data. 

```{r}
ranger_test_prediction <- predict(
  ranger_fit,
  new_data = data_testing)

ranger_test_pred<-data_testing %>% 
  mutate(.pred = ranger_test_prediction$.pred)
  
  
ranger_test_pred %>%
  ggplot(aes(x = PROFIT,
             y = .pred)) +
  geom_point(alpha = .5,
             size = .5) +
  geom_smooth(se = FALSE) +
  geom_abline(slope = 1,
              intercept = 0,
              color = "darkred") +
  labs(x = "Actual Return",
       y = "Predicted Return")

```

## Interpretable Machine Learning 

Based on the box-plot and the histogram below, the residuals mostly lie between -50 to 50. But there are also a few outliners that can go up to 400. 

```{r}
rf_explain <- 
  explain_tidymodels(
    model = ranger_fit,
    data = data_training %>% select(-PROFIT), 
    y = data_training %>%  pull(PROFIT),
    label = "rf"
  )
rf_mod_perf <-  model_performance(rf_explain)

hist_plot <- 
  plot(rf_mod_perf, 
       geom = "histogram")
box_plot <-
  plot(rf_mod_perf, 
       geom = "boxplot")

hist_plot
box_plot
```
According to the feature importance bar chart below, we can see that the top three important features are market cap, earnings, M1SL_PC1, and GDP_PC1. 

```{r}
set.seed(10) #since we are sampling & permuting, we set a seed so we can replicate the results
rf_var_imp <- 
  model_parts(
    rf_explain
    )
plot(rf_var_imp, show_boxplots = TRUE)
```





